{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sINzyse42sj3"
      },
      "source": [
        "Import dataset: [credit card](https://www.kaggle.com/mlg-ulb/creditcardfraud#creditcard.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbnJB3_g2jdl"
      },
      "source": [
        "# Set up Kaggle\n",
        "\n",
        "import json\n",
        "token = {\"username\":\"ngungu46\",\"key\":\"bdc647a0e1beed7a20e36fd4df045d7a\"}\n",
        "\n",
        "!mkdir -p '/root/.kaggle/'\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "  json.dump(token, file)\n",
        "\n",
        "!chmod 600 $'/root/.kaggle/kaggle.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_MWaYS321uU",
        "outputId": "a9cfb3e1-8a5c-4b66-b50e-5bbce512cba1"
      },
      "source": [
        "# Download Credit Card Fraud Detection\n",
        "\n",
        "!mkdir -p /content/dataset\n",
        "%cd /content/dataset/\n",
        "!kaggle datasets download -d mlg-ulb/creditcardfraud"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset\n",
            "creditcardfraud.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aEZXJPt24gP",
        "outputId": "e822dafd-08a6-4177-af90-d345b06cf849"
      },
      "source": [
        "# Unzip\n",
        "\n",
        "!unzip -q creditcardfraud.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace creditcard.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AQoawDH26DL",
        "outputId": "2e872f5e-4f91-43da-bd4e-16d172a2b4b2"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "catboost_info  creditcard.csv  creditcardfraud.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwzsMfZMDVYO"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jDfvotp27rz"
      },
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhHPKy5b29Sp"
      },
      "source": [
        "Check data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8deJ1dhu3F4-",
        "outputId": "d3295872-c4f1-4135-f931-fd694530d9aa"
      },
      "source": [
        "#Check imported data\n",
        "\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "print(\"First 10 lines: \")\n",
        "print(df.head(10))\n",
        "\n",
        "print(\"Describe: \")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"Information: \")\n",
        "print(df.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(284807, 31)\n",
            "First 10 lines: \n",
            "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
            "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
            "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
            "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
            "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
            "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
            "5   2.0 -0.425966  0.960523  1.141109  ...  0.253844  0.081080    3.67      0\n",
            "6   4.0  1.229658  0.141004  0.045371  ...  0.034507  0.005168    4.99      0\n",
            "7   7.0 -0.644269  1.417964  1.074380  ... -1.206921 -1.085339   40.80      0\n",
            "8   7.0 -0.894286  0.286157 -0.113192  ...  0.011747  0.142404   93.20      0\n",
            "9   9.0 -0.338262  1.119593  1.044367  ...  0.246219  0.083076    3.68      0\n",
            "\n",
            "[10 rows x 31 columns]\n",
            "Describe: \n",
            "                Time            V1  ...         Amount          Class\n",
            "count  284807.000000  2.848070e+05  ...  284807.000000  284807.000000\n",
            "mean    94813.859575  3.919560e-15  ...      88.349619       0.001727\n",
            "std     47488.145955  1.958696e+00  ...     250.120109       0.041527\n",
            "min         0.000000 -5.640751e+01  ...       0.000000       0.000000\n",
            "25%     54201.500000 -9.203734e-01  ...       5.600000       0.000000\n",
            "50%     84692.000000  1.810880e-02  ...      22.000000       0.000000\n",
            "75%    139320.500000  1.315642e+00  ...      77.165000       0.000000\n",
            "max    172792.000000  2.454930e+00  ...   25691.160000       1.000000\n",
            "\n",
            "[8 rows x 31 columns]\n",
            "Information: \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "2Mt6SP6fECRP",
        "outputId": "a81fa928-16c4-49b0-e360-5336450dc8a4"
      },
      "source": [
        "print(\"Class\")\n",
        "fig, ax = plt.subplots(1,1)\n",
        "ax.pie(df.Class.value_counts(), autopct='%1.1f%%',labels=['Real','Fraud'], colors=['b','g'])\n",
        "plt.axis('equal')\n",
        "plt.ylabel('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, '')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdzRKHmjDG84"
      },
      "source": [
        "Data Processing before treating imbalancy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTu-lvPKDFq3"
      },
      "source": [
        "X = df.drop('Class', axis = 1)\n",
        "y = df['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyUKUe4QDmyV",
        "outputId": "c559a970-95a6-431f-e967-feca3c4e9923"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "test_predict = gnb.predict(X_test)\n",
        "print(confusion_matrix(y_test, test_predict))\n",
        "print(classification_report(y_test, test_predict))\n",
        "print(accuracy_score(y_test, test_predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[56478   375]\n",
            " [   30    79]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     56853\n",
            "           1       0.17      0.72      0.28       109\n",
            "\n",
            "    accuracy                           0.99     56962\n",
            "   macro avg       0.59      0.86      0.64     56962\n",
            "weighted avg       1.00      0.99      1.00     56962\n",
            "\n",
            "0.9928899968399986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ep1cL_HDxf0"
      },
      "source": [
        "Precision score of 1 is low due to imbalanced data between 1 and 0. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MYjHwBuAwyW"
      },
      "source": [
        "Processing Imbalanced Data with SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "3flFdAbJA0pM",
        "outputId": "b8e60522-6b4e-4f87-b0f3-56bbd5ad8188"
      },
      "source": [
        "X = df.drop('Class', axis = 1)\n",
        "y = df['Class']\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X, y)\n",
        "sns.countplot(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9dd53b4290>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "ZP12yueHBg_h",
        "outputId": "e2e7c890-7c9d-4520-d0f1-f218dc5f3fdd"
      },
      "source": [
        "# create new balanced data frame\n",
        "\n",
        "X = pd.DataFrame(X)\n",
        "y = pd.DataFrame(y)\n",
        "df1 = pd.concat([X,y], axis = 1)\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.620000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.690000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.660000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.500000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.990000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568625</th>\n",
              "      <td>75769.125144</td>\n",
              "      <td>-1.756669</td>\n",
              "      <td>1.072407</td>\n",
              "      <td>-1.716714</td>\n",
              "      <td>0.529116</td>\n",
              "      <td>-0.584399</td>\n",
              "      <td>0.283902</td>\n",
              "      <td>-2.068326</td>\n",
              "      <td>-1.456660</td>\n",
              "      <td>-2.238380</td>\n",
              "      <td>-1.240357</td>\n",
              "      <td>1.120488</td>\n",
              "      <td>-2.113721</td>\n",
              "      <td>-0.286074</td>\n",
              "      <td>-1.831546</td>\n",
              "      <td>0.808505</td>\n",
              "      <td>-0.649936</td>\n",
              "      <td>-1.092447</td>\n",
              "      <td>-1.001482</td>\n",
              "      <td>2.438291</td>\n",
              "      <td>0.651029</td>\n",
              "      <td>-0.596102</td>\n",
              "      <td>0.820033</td>\n",
              "      <td>-0.193992</td>\n",
              "      <td>-1.055388</td>\n",
              "      <td>0.037034</td>\n",
              "      <td>0.094296</td>\n",
              "      <td>-0.090739</td>\n",
              "      <td>0.221103</td>\n",
              "      <td>69.657369</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568626</th>\n",
              "      <td>157055.862162</td>\n",
              "      <td>-0.065519</td>\n",
              "      <td>2.966387</td>\n",
              "      <td>-3.280189</td>\n",
              "      <td>5.058299</td>\n",
              "      <td>1.518023</td>\n",
              "      <td>-1.166053</td>\n",
              "      <td>-0.000948</td>\n",
              "      <td>0.278350</td>\n",
              "      <td>-3.105715</td>\n",
              "      <td>-1.958180</td>\n",
              "      <td>2.091241</td>\n",
              "      <td>-3.191926</td>\n",
              "      <td>-0.548354</td>\n",
              "      <td>-6.059570</td>\n",
              "      <td>-0.215141</td>\n",
              "      <td>-0.007729</td>\n",
              "      <td>0.517115</td>\n",
              "      <td>1.164554</td>\n",
              "      <td>-0.274329</td>\n",
              "      <td>0.256679</td>\n",
              "      <td>0.246454</td>\n",
              "      <td>0.044954</td>\n",
              "      <td>-0.319497</td>\n",
              "      <td>-0.705422</td>\n",
              "      <td>0.174095</td>\n",
              "      <td>0.430435</td>\n",
              "      <td>0.135508</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>3.480627</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568627</th>\n",
              "      <td>129693.391703</td>\n",
              "      <td>0.005671</td>\n",
              "      <td>2.401023</td>\n",
              "      <td>-5.467898</td>\n",
              "      <td>3.734186</td>\n",
              "      <td>0.310284</td>\n",
              "      <td>-1.884870</td>\n",
              "      <td>-1.741758</td>\n",
              "      <td>0.607855</td>\n",
              "      <td>-1.520919</td>\n",
              "      <td>-4.040079</td>\n",
              "      <td>3.337554</td>\n",
              "      <td>-4.313216</td>\n",
              "      <td>-1.346538</td>\n",
              "      <td>-7.829729</td>\n",
              "      <td>0.315315</td>\n",
              "      <td>-2.575879</td>\n",
              "      <td>-1.407305</td>\n",
              "      <td>0.309008</td>\n",
              "      <td>1.599793</td>\n",
              "      <td>0.197444</td>\n",
              "      <td>0.298934</td>\n",
              "      <td>-0.113569</td>\n",
              "      <td>0.246248</td>\n",
              "      <td>-0.031411</td>\n",
              "      <td>-0.285298</td>\n",
              "      <td>-0.338476</td>\n",
              "      <td>0.214930</td>\n",
              "      <td>-0.114499</td>\n",
              "      <td>1.652168</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568628</th>\n",
              "      <td>151964.419461</td>\n",
              "      <td>-5.016982</td>\n",
              "      <td>3.277259</td>\n",
              "      <td>-5.916985</td>\n",
              "      <td>4.772103</td>\n",
              "      <td>-1.054566</td>\n",
              "      <td>-1.822472</td>\n",
              "      <td>-3.497468</td>\n",
              "      <td>-0.157590</td>\n",
              "      <td>-1.797479</td>\n",
              "      <td>-3.782623</td>\n",
              "      <td>4.072809</td>\n",
              "      <td>-7.429657</td>\n",
              "      <td>-0.470340</td>\n",
              "      <td>-10.982398</td>\n",
              "      <td>1.587341</td>\n",
              "      <td>-4.710093</td>\n",
              "      <td>-6.428040</td>\n",
              "      <td>-1.854728</td>\n",
              "      <td>1.699760</td>\n",
              "      <td>-0.435142</td>\n",
              "      <td>1.218098</td>\n",
              "      <td>0.379955</td>\n",
              "      <td>0.024429</td>\n",
              "      <td>0.246895</td>\n",
              "      <td>-0.241672</td>\n",
              "      <td>0.412719</td>\n",
              "      <td>-2.224614</td>\n",
              "      <td>1.122735</td>\n",
              "      <td>0.409461</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568629</th>\n",
              "      <td>129084.460132</td>\n",
              "      <td>-1.852661</td>\n",
              "      <td>-1.481375</td>\n",
              "      <td>-3.434959</td>\n",
              "      <td>0.543583</td>\n",
              "      <td>0.016950</td>\n",
              "      <td>-0.447973</td>\n",
              "      <td>3.578292</td>\n",
              "      <td>-1.234293</td>\n",
              "      <td>0.044925</td>\n",
              "      <td>-0.279521</td>\n",
              "      <td>-1.379459</td>\n",
              "      <td>-0.631310</td>\n",
              "      <td>-0.273684</td>\n",
              "      <td>0.403010</td>\n",
              "      <td>0.146987</td>\n",
              "      <td>-0.944651</td>\n",
              "      <td>-0.459405</td>\n",
              "      <td>-0.021743</td>\n",
              "      <td>1.038537</td>\n",
              "      <td>-1.530013</td>\n",
              "      <td>0.036213</td>\n",
              "      <td>1.754454</td>\n",
              "      <td>-0.146863</td>\n",
              "      <td>-0.549431</td>\n",
              "      <td>-0.923660</td>\n",
              "      <td>-0.337158</td>\n",
              "      <td>0.710482</td>\n",
              "      <td>0.363433</td>\n",
              "      <td>701.131596</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>568630 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0         1         2   ...        28          29  0 \n",
              "0            0.000000 -1.359807 -0.072781  ... -0.021053  149.620000   0\n",
              "1            0.000000  1.191857  0.266151  ...  0.014724    2.690000   0\n",
              "2            1.000000 -1.358354 -1.340163  ... -0.059752  378.660000   0\n",
              "3            1.000000 -0.966272 -0.185226  ...  0.061458  123.500000   0\n",
              "4            2.000000 -1.158233  0.877737  ...  0.215153   69.990000   0\n",
              "...               ...       ...       ...  ...       ...         ...  ..\n",
              "568625   75769.125144 -1.756669  1.072407  ...  0.221103   69.657369   1\n",
              "568626  157055.862162 -0.065519  2.966387  ...  0.001264    3.480627   1\n",
              "568627  129693.391703  0.005671  2.401023  ... -0.114499    1.652168   1\n",
              "568628  151964.419461 -5.016982  3.277259  ...  1.122735    0.409461   1\n",
              "568629  129084.460132 -1.852661 -1.481375  ...  0.363433  701.131596   1\n",
              "\n",
              "[568630 rows x 31 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtkgYtdeCorj",
        "outputId": "315502ee-6315-4097-d96b-1c07368a83a1"
      },
      "source": [
        "df1.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     0\n",
              "1     0\n",
              "2     0\n",
              "3     0\n",
              "4     0\n",
              "5     0\n",
              "6     0\n",
              "7     0\n",
              "8     0\n",
              "9     0\n",
              "10    0\n",
              "11    0\n",
              "12    0\n",
              "13    0\n",
              "14    0\n",
              "15    0\n",
              "16    0\n",
              "17    0\n",
              "18    0\n",
              "19    0\n",
              "20    0\n",
              "21    0\n",
              "22    0\n",
              "23    0\n",
              "24    0\n",
              "25    0\n",
              "26    0\n",
              "27    0\n",
              "28    0\n",
              "29    0\n",
              "0     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Pa3fmy3O01"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llt_vib24m7Q",
        "outputId": "75344546-6d3f-4295-d4f3-4ef0a5ad8339"
      },
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "test_predict = gnb.predict(X_test)\n",
        "print(confusion_matrix(y_test, test_predict))\n",
        "print(classification_report(y_test, test_predict))\n",
        "print(accuracy_score(y_test, test_predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[56374   407]\n",
            " [14804 42141]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.99      0.88     56781\n",
            "           1       0.99      0.74      0.85     56945\n",
            "\n",
            "    accuracy                           0.87    113726\n",
            "   macro avg       0.89      0.87      0.86    113726\n",
            "weighted avg       0.89      0.87      0.86    113726\n",
            "\n",
            "0.8662487030230555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xali6NYAEaFs"
      },
      "source": [
        "The result is now better with higher accuracy for 1, but the accuracy for predicting 0 decreases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwU9BwKGFXt-"
      },
      "source": [
        "The below compared this method to the accuracy achieved when we use Logistic regression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKpEU1p-46PI",
        "outputId": "c818754e-6d10-4ead-90fd-d4c0379e15a7"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "test_predict = log_reg.predict(X_test)\n",
        "print(confusion_matrix(y_test, test_predict))\n",
        "print(classification_report(y_test, test_predict))\n",
        "print(accuracy_score(y_test, test_predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[55666  1115]\n",
            " [ 2086 54859]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97     56781\n",
            "           1       0.98      0.96      0.97     56945\n",
            "\n",
            "    accuracy                           0.97    113726\n",
            "   macro avg       0.97      0.97      0.97    113726\n",
            "weighted avg       0.97      0.97      0.97    113726\n",
            "\n",
            "0.9718534020364736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCKh3l58GeU5"
      },
      "source": [
        "Precision score improves significantly."
      ]
    }
  ]
}